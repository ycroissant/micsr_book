---
editor: 
  markdown: 
    wrap: 72
---

```{r }
#| include: false
source("../_commonR.R")
```

# Non-spherical disturbances {#sec-non_spherical}


In the first part of the book, we considered a model of the form:
$y_n = \alpha + \beta x_n + \epsilon_n$ with the hypothesis that the errors where homoskedastic: $\mbox{V}(\epsilon_n) = \sigma_\epsilon ^ 2$ and uncorrelated: $\mbox{E}(\epsilon_n \epsilon_m) = 0 \; \forall n \neq m$.In this case, the errors (or disturbances) are
**spherical** and the covariance matrix of the errors $\Omega$ is, up to
a multiplicative constant $\sigma_\epsilon^2$, the identity matrix:

$$
\Omega = \mbox{V}(\epsilon) = \mbox{E}(\epsilon\epsilon^\top) = \sigma_\epsilon ^ 2 I
$$

In this chapter, we analyze cases where these hypothesis are violated.
This has the following consequences concerning the results established
in the first part of the book:

-   the OLS estimator is still consistent: this means that $\hat{\beta}$
    estimates consistently $\beta$ and $\hat{\epsilon}$ estimates also
    consistently $\epsilon$; this is an important result as the
    residuals of the OLS estimator can therefore be used to test whether
    the errors are spherical or not,
-   the OLS estimator is no longer BLUE, ie it is no longer the best
    linear unbiased estimator,
-   there is another linear unbiased estimator (the **generalized least
    squares**, abbreviated as **GLS**) which is more efficient (which
    means that it has a smaller variance) than the OLS estimator and
    which is the BLUE estimator when the errors are non-spherical,\index[general]{generalized least squares}
-   the simple formula for the variance of the OLS estimator,
    $\mbox{V}(\hat{\beta}) = \sigma_\epsilon ^ 2 \left(\tilde{X}^\top \tilde{X}\right)^{-1}$
    is no longer an unbiased estimator of the true covariance matrix of
    the OLS estimator. However, as we'll see in this chapter,
    **sandwich** estimators, which are unbiased, can be used.\index[general]{sandwich}

@sec-situations_non_spher reviews some important cases where the errors
are non-spherical. @sec-test_non_spher presents tests that
enable to detect whether the errors are spherical or not. @sec-sandwich presents robust estimators of the
variance of the OLS estimators. Finally, section @sec-gls is devoted to the GLS
estimator.

## Situations where the errors are non-spherical {#sec-situations_non_spher}

As stated previously, the hypothesis of spherical disturbances implies
that the errors are homoskedastic and uncorrelated. We'll describe in the next three subsections important situations where this hypothesis is violated. In each case, we'll establish the
expression of the matrix of covariance of the errors $\Omega$ and, 
for a reason that will be clear in the subsequent sections, we'll also
compute the inverse of this matrix.

### Heteroskedasticity
\index[general]{heteroskedasticity|(}
In a linear model: $y_n = \alpha + \beta x_n + \epsilon_n$, 

heteroskedasticity occurs when the conditional variance of the response
$y_n$ (the variance of $\epsilon_n$) is not a constant. 
As an example, @HOUT:51\index[author]{Houthakker} analyzed electric consumption in the United Kingdom and his data set (called `uk_elec`)\idxdata{uk\_elec}{micsr.data} is a sample of 42 British cities.^[This data set is used extensively by @BERN:91\index[author]{Berndt}, chapter 7.] The response is the per capita consumption in kilowatt hours. Denoting $c_{ni}$ the consumption of an individual $i$ in city $n$, the response is then $y_n =\frac{1}{I_n}\sum_i^{I_n} c_{ni}$ where $I_n$ is the total number of consumers in city $n$. Then, if the standard
deviation of the individual consumption is $\sigma_c$ (the same in every
city), the variance of $y_n$ is $\sigma_c ^ 2 / I_n$ and therefore
depends on the number of consumer units of the city. Even
if covariates are taken into account, it is doubtful that the
conditional variance of $y$ will be the same for every city and a more
reasonable hypothesis is that, as the unconditional variance, it is
inversely proportional to the number of consumer units. With heteroskedastic but uncorrelated errors, the matrix of covariance of the errors is a diagonal matrix with non-constant diagonal terms:

$$
\Omega= 
\left(
\begin{array}{cccc}
\sigma_{1} ^ 2 & 0 & \ldots & 0 \\
0 & \sigma_{2} ^ 2 & \ldots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \ldots & \sigma_N ^ 2
\end{array}
\right)
$$ {#eq-matheterosc}

The inverse of $\Omega$ is easily obtained:

$$
\Omega ^ {-1}= 
\left(
\begin{array}{cccc}
1 / \sigma_{1} ^ 2 & 0 & \ldots & 0 \\
0 & 1 / \sigma_{2} ^ 2 & \ldots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \ldots & 1/ \sigma_N ^ 2
\end{array}
\right)
$$

and, more generally, $\Omega^{r}$ ($r$ being any integer or rational number) is a diagonal matrix with typical
element $\left(\sigma_n^2\right)^r$.
\index[general]{heteroskedasticity|)}

### Correlation of the errors {#sec-error_component}
\index[general]{correlated errors|(}

Consider now the case where we have several observations from the same
**entity**. An example is the case where the unit of
observation is the individual, but siblings are observed. In this case, each observation is doubly indexed,
the first index being the family and the second one the rank of birth in the sibling.
Another very important case is when the same individual
(in a wide sense, it can be an household, a firm, a country, ...) is
observed several times, for example for different periods like years
or months. Such a data set is called a **panel data**. 
\index[general]{panel data|(}
In the subsequent sections, we'll use two data sets. The first one, called `twins`, 
is from @BONJ:CHERK:KASK:03\idxdata{twins}{micsr.data}
\index[author]{Bonjour}\index[author]{Cherkas}\index[author]{Haskel}\index[author]{Hawkes}\index[author]{Spector} who studied the return to education with a
sample of twins. The sample contains 428 observations (214 couples
of twins) and it is reasonable to assume that, for a given pair of twins,
the two errors are correlated as they partly contain unobserved
characteristics that are common to both twins.
The second one, called `tobinq`, is from @SCHA:90\index[author]{Schaller} who tested the relevance of Tobins' Q theory of investment by
regressing the investment rate (the ratio of the investment and the
stock of capital) to Tobin's Q, which is the ratio of the value of the
firm and the stock of capital. The data set is a panel of 188 firms observed for 35 years (from 1951 to 1985).\idxdata{tobinq}{micsr.data}

For such data, we'll denote $n = 1, 2, \ldots N$ the entity /
individual index and $t = 1, 2, \ldots T$ the index of the observation in the
entity / the time period. Then, the simple linear model can be written:
$y_{nt} = \alpha + \beta x_{nt} + \epsilon_{nt}$
and it is useful to write the error term as the sum of two
components:

- an entity / individual effect $\eta_n$ and,
- an idiosyncratic effect $\nu_{nt}$.

We therefore have $\epsilon_{nt} = \eta_n + \nu_{nt}$. This leads to the
so-called **error-component** model, which can be easily analyzed with the following
hypothesis:\index[data]{error component model}

-   the two components are homoskedastic and uncorrelated:
    $\mbox{V}(\eta_n) = \sigma_{\eta} ^ 2, \forall n$,
    $\mbox{V}(\nu_{nt}) = \sigma_{\nu} ^ 2, \forall n, t$ and
    $\mbox{cov}(\eta_n, \nu_{nt}) = 0, \forall n, t$,
-   the idiosyncratic terms for the same entity are uncorrelated:
    $\mbox{E}(\nu_{nt}\nu_{ns})=0\; \forall \; n, t \neq s$,
-   the two components of the errors are uncorrelated for two
    observations of different entities
    $\mbox{cov}(\nu_{nt}, \nu_{ms})=\mbox{cov}(\eta_n, \eta_m)=0\; \forall \; n \neq m, t, s$.

With these hypothesis, we have:

$$
\left\{
\begin{array}{rcl}
\mbox{E}(\epsilon_{nt}^2) &=& \sigma_\eta ^ 2 + \sigma_\nu ^ 2 \\
\mbox{E}(\epsilon_{nt}\epsilon_{mt}) &=& \sigma_\eta ^ 2 \\
\mbox{E}(\epsilon_{nt}\epsilon_{ms}) &=& 0;\ \forall \; n \neq m\\
\end{array}
\right.
$$ 

and $\Omega$ is a block-diagonal matrix with identical blocks. For
example, with $N = 2$ and $T = 3$:

$$
\Omega = 
\left(
\begin{array}{cccccc}
\sigma_\eta ^ 2 + \sigma_\nu ^ 2 & \sigma_\eta ^ 2 & \sigma_\eta ^ 2 & 0 & 0 & 0 \\
\sigma_\eta ^ 2  & \sigma_\eta ^ 2 + \sigma_\nu ^ 2 & \sigma_\eta ^ 2 & 0 & 0 & 0 \\
\sigma_\eta ^ 2  & \sigma_\eta ^ 2  & \sigma_\eta ^ 2 + \sigma_\nu ^ 2 & 0 & 0 & 0 \\
0 & 0 & 0 & \sigma_\eta ^ 2 + \sigma_\nu ^ 2 & \sigma_\eta ^ 2 & \sigma_\eta ^ 2 \\
0 & 0 & 0 & \sigma_\eta ^ 2  & \sigma_\eta ^ 2 + \sigma_\nu ^ 2 & \sigma_\eta ^ 2  \\
0 & 0 & 0 & \sigma_\eta ^ 2  & \sigma_\eta ^ 2  & \sigma_\eta ^ 2 + \sigma_\nu ^ 2\\
\end{array}
\right)
$$

The blocks of this matrix can be written as, denoting $j$ a vector of
ones and $J=jj^\top$ a square matrix of 1:

$$
\sigma^2_\nu I_T + \sigma ^ 2_\eta J_T
$$

and, using Kronecker product, $\Omega$ is:\index[general]{Kronecker product}

$$
\Omega = I_N \otimes \left(\sigma^2_\nu I_T + \sigma ^ 2_\eta J_T\right) = \sigma ^ 2_\nu I_N + \sigma_\eta ^ 2 I_N \otimes J_T
$$

Another equivalent expression which will prove to be particularly useful
is:

$$
\Omega = \sigma ^ 2_\nu \left(I_N - I_N \otimes J_T / T\right)+ (T \sigma_\eta ^ 2 + \sigma ^ 2_\nu) \left(I_N \otimes J_T / T\right) = \sigma ^ 2_\nu W + \sigma_\iota^2 B
$$ {#eq-omega_panel}

where $\sigma_\iota ^ 2 = T \sigma_\eta ^ 2 + \sigma ^ 2_\nu$. With our
$N=2$ and $T=3$ simple case, the two matrices are:

$$
W =
\left(
\begin{array}{cccccc}
2/3 & - 1/3 & - 1/3 & 0 & 0 & 0 \\
- 1/3 & 2/3 & - 1/3 & 0 & 0 & 0 \\
- 1/3 & - 1/3 & 2/3 & 0 & 0 & 0 \\
0 & 0 & 0 & 2/3 & - 1/3 & - 1/3 \\
0 & 0 & 0 & - 1/3 & 2/3 & - 1/3 \\
0 & 0 & 0 & - 1/3 & - 1/3 & 2/3 \\
\end{array}
\right)
$$

and:

$$
B = 
\left(
\begin{array}{cccccc}
- 1/3 & - 1/3 & - 1/3 & 0 & 0 & 0 \\
- 1/3 & - 1/3 & - 1/3 & 0 & 0 & 0 \\
- 1/3 & - 1/3 & - 1/3 & 0 & 0 & 0 \\
0 & 0 & 0 & - 1/3 & - 1/3 & - 1/3 \\
0 & 0 & 0 & - 1/3 & - 1/3 & - 1/3 \\
0 & 0 & 0 & - 1/3 & - 1/3 & - 1/3 \\
\end{array}
\right)
$$

The first matrix is called the **within** matrix. Premultiplying a
vector by $W$ transforms it as deviations from the individual means. The
second one is called the **between** matrix and premultiplying a vector
by $B$ transforms it as a vector of individual means. These two symmetric
matrices have interesting properties:
\index[general]{within matrix}\index[general]{between matrix}

-   they are **idempotent**, which means that $B B = B$ and $W W =W$.
    For example $W (Wz) = Wz$, as taking the deviations from the
    individual means of a vector of deviations from the individual means
    leaves this vector unchanged,\index[general]{idempotent matrix}
-   they are **orthogonal**, which means that $W B = B W = 0$. For
    example $W (Bz) = 0$ because the deviations from the individual
    means of a vector of individual means are zero,
-   they sum to the **identity matrix**, $W + B = I$. $Wz + Bz=z$,
    because the sum of the deviations from the individual means of a
    vector and its individual means is the vector itself.

$W$ and $B$ therefore perform an **orthogonal decomposition** of a
vector. One advantage of this decomposition is that it is very easy to
obtain powers of $\Omega$. For example, the inverse of $\Omega$ is:

$$
\Omega ^ {-1} = \frac{1}{\sigma_\nu ^ 2} W + \frac{1}{\sigma_\iota ^ 2}B
$$

and, more generally, for any power $r$ (either an integer or a
rational):

$$
\Omega ^ {r} = {\sigma_\nu ^ 2} ^ r W + {\sigma_\iota ^ 2} ^ rB
$$ {#eq-poweromegaec}
\index[general]{correlated errors|)}
\index[general]{panel data|)}

### System of equations

\index[general]{system estimation}
We have seen in @sec-system_equation that, 
for example in fields such as consumption or production analysis, it is more relevant to consider the estimation of system of equations instead of the estimation of a single equation. In matrix form, the model corresponding to the whole system was presented in @eq-system_equation. We've seen in @sec-system_equation than a first advantage of considering the whole system of equations, and not an equation in isolation, was that restrictions on coefficients that concern different equations can be taken into account using the constrained least squares estimator.

The second advantage is that, if the errors of the different equations for the same observation are correlated, these correlations can be taken into account if the whole system of equations is considered.
Denoting $\epsilon_l$ the vector of length $N$ containing the errors for the $l$^th^ equation and $\Xi = (\epsilon_1, \epsilon_2, \ldots \epsilon_L)$ the $N\times L$ matrix containing errors for the whole system, the covariance matrix of the errors of the system is:

$$
\Omega = 
\mbox{E}(\Xi \Xi^\top)
=\mbox{E}
\left(
  \begin{array}{cccc}
    \epsilon_1\epsilon_1^\top & \epsilon_1 \epsilon_2^\top & \ldots & \epsilon_1 \epsilon_L^\top \\
    \epsilon_2\epsilon_1^\top & \epsilon_2 \epsilon_2^\top & \ldots & \epsilon_2 \epsilon_L^\top \\
    \vdots & \vdots & \ddots & \vdots \\
    \epsilon_L\epsilon_1^\top & \epsilon_L \epsilon_2^\top & \ldots & \epsilon_L \epsilon_L^\top \\
  \end{array}
\right)
$$
Assume that the errors of two equations $l$ and $m$ for the same
observation are correlated and that the covariance, denoted
$\sigma_{lm}$, is constant. The variance of errors for each equation $l$ is denoted $\sigma_{ll}$ and may be different from one equation to another. Moreover, we assume that errors for different individuals are uncorrelated. With these hypothesis, the covariance matrix is, denoting $I$ the identity matrix of dimension $N$:

$$
\Omega=
\left(
\begin{array}{cccc}
  \sigma_{11} I & \sigma_{12} I & \ldots &\sigma_{1L} I \\
  \sigma_{12} I & \sigma_{22} I & \ldots &\sigma_{2L} I \\
  \vdots & \vdots & \ddots & \vdots \\
  \sigma_{1L} I & \sigma_{2L} I & \ldots & \sigma_{LL} I
  \end{array}
\right)
$$

Denoting $\Sigma$ the $L\times L$ matrix of inter-equations variances and covariances, we have:

$$
\Sigma=
\left(
  \begin{array}{cccc}
  \sigma_{11} & \sigma_{12} & \ldots &\sigma_{1L} \\
  \sigma_{12} & \sigma_{22} & \ldots &\sigma_{2L} \\
  \vdots & \vdots & \ddots & \vdots \\
  \sigma_{1L} & \sigma_{2L} & \ldots & \sigma_{LL}
  \end{array}
\right)
$$

and $\Omega=\Sigma \otimes I$. The inverse of the covariance matrix of the errors is easily obtained as
it requires only to compute the inverse of $\Sigma$: $\Omega ^ {-1} = \Sigma ^ {-1} \otimes I$

## Testing for non-spherical disturbances {#sec-test_non_spher}

Numerous tests have been proposed to investigate whether, in different
contexts, the disturbances are spherical or note. Among them, we'll
present a family of tests that are based on OLS residuals. Even if the disturbances are non-spherical, OLS is
a consistent estimator and therefore OLS's residuals are a consistent
estimate of the errors of the model. Therefore one can use these
residuals to analyze the unknown features of the errors.

### Testing for heteroskedasticity
\index[general]{test!heteroskedasticity|(}
\index[general]{Breusch Pagan test!heteroskedasticity|(}

[@BREU:PAGA:79]\index[author]{Breusch}\index[author]{Pagan} consider the following heteroskedastic model: $y_n = \gamma ^ \top z_n + \epsilon_n$ with $\epsilon_n \sim \mathcal{N}(0, \sigma_n ^ 2)$. Assume that $\sigma_n^2$ is a
function of a set of $J$ covariates denoted $w_n$:

$$
\sigma_n ^ 2 = h(\delta ^ \top w_n)
$$ 

The first element of $w$ is 1, so that the homoskedasticity
hypothesis is that all the elements of $\delta$ except the first one are
0: $\delta_0^\top = (\alpha, 0, \ldots, 0)$, so that $\sigma ^ 2_n = h(\delta_0) = h_0$.
The log-likelihood function is:

$$
\ln L = -\frac{N}{2}\ln 2\pi - \frac{1}{2} \sum_n \ln \sigma_n ^ 2 - \frac{1}{2}\sum_n \frac{(y_n - \gamma z_n)^2}{\sigma_n ^ 2}
$$ The derivative of $\ln L$ with $\delta$ is, denoting $h'_n = \frac{\partial h}{\partial \delta}(w_n)$:

$$
\frac{\partial \ln L}{\partial \delta} = \frac{1}{2}\sum_n \left(\frac{\epsilon ^ 2}{\sigma_n ^ 4} - \frac{1}{\sigma_n ^ 2}\right) h'_n w_n
$$ {#eq-gen_score_bp_heter}

With the homoskedasticity hypothesis, $\sigma_n = \sigma$ and $h'_n = h_n'(\delta_0) = h_0'$ and @eq-gen_score_bp_heter simplifies to:

$$
d = \frac{\partial \ln L}{\partial \delta}(\delta_0) = \frac{h'_0}{2\sigma ^ 2}\sum_n \left(\frac{\epsilon_n ^ 2}{\sigma ^ 2} - 1\right) w_n
$$ 
The second derivatives are:

$$ 
\frac{\partial \ln ^ 2 L}{\partial \delta \partial\delta ^ \top} =
\frac{1}{2}\sum_n\left[ h_n''\left(\frac{\epsilon_n ^ 2}{\sigma_n ^ 4}-
\frac{1}{\sigma_n ^ 2} \right)- h_n^{'2} \left(\frac{2 \epsilon_n ^
2}{\sigma_n ^ 6}- \frac{1}{\sigma_n ^ 4} \right)\right]w_n w_n'
$$

To get the information matrix, we take the expectation of the opposite
of this matrix. If the errors are homoskedastic, the first term disappears and $h'_n = h'_0$ so that:

$$
I_0 = \mbox{E}\left(- \frac{\partial \ln ^ 2 L}{\partial \delta \partial\delta ^ \top}(\delta_0)\right) = \frac{h_0^{'2}}{2\sigma ^ 4}\sum_nw_n w_n'
$$ {#eq-info}

Denoting $\hat{\epsilon}$ the vector of OLS residuals and $\hat{\sigma} ^ 2 = \hat{\epsilon} ^ \top \hat{\epsilon} / N$ the estimate of $\sigma ^ 2$, the estimated score is:

$$
\hat{d} = \frac{h'_0}{2\hat{\sigma} ^ 2}\sum_n \left(\frac{\hat{\epsilon}_n ^ 2}{\hat{\sigma} ^ 2} - 1\right) w_n
$$

and the test statistic is the quadratic form of $\hat{d}$ with the
inverse of its variance given by @eq-info:

$$
LM = \frac{1}{2} \left[\sum_n \left(\frac{\hat{\epsilon}_n ^ 2}{\hat{\sigma} ^ 2} - 1\right) w_n^\top\right] \left(\sum_n w_n w_n ^ \top\right)^{-1}
\left[\sum_n \left(\frac{\hat{\epsilon}_n ^ 2}{\hat{\sigma} ^ 2} - 1\right) w_n\right]
$$ 
or, in matrix form, denoting $f$ the $N$-length vector with typical
element $\left(\frac{\hat{\epsilon}_n ^ 2}{\hat{\sigma} ^ 2} - 1\right)$
and $W$ the matrix of covariates:

$$
LM =\frac{1}{2}f^\top W (W^\top W)^{-1} W f = \frac{1}{2}f ^ \top P_W f
$$ {#eq-bp_heteros_ESS}

which is half the explained sum of squares of a regression of $f_n$ on
$w_n$ and is $\chi ^ 2$ with $J$ degrees of freedom in case of homoskedasticity.
Note also that
$f^\top f / N = \sum_n \left(\frac{\hat{\epsilon}_n ^ 2}{\hat{\sigma} ^ 2} - 1\right) ^ 2 / N= \sum_n\left(\frac{\hat{\epsilon}_n ^ 4}{\hat{\sigma} ^ 4} + 1 - 2 \frac{\hat{\epsilon}_n ^ 2}{\hat{\sigma} ^ 2}\right) / N$
is the total sum of squares divided by $N$. It converges to
2 as the first term is the fourth center moment of a normal variable, which is 3.. Therefore, a second version of the statistic can be computed as $N$ times the R^2^ of a regression of the first-step residuals on $w$:

$$
N R^2 = \frac{f^\top P_W f}{f'f / N}
$$

@WHIT:80\index[author]{White} proposed a test that is directly linked to its proposition of
the heteroskedasticity-robust matrix of covariance of the OLS
estimates. This matrix depends on the squares and on the
cross-products of the covariates. Therefore, he proposed to run a
regression of the squares of the first-step residuals on the covariates,
their squares and their cross-product. $N R^2$ of this regression is
asymptotically distributed as a $\chi ^ 2$ with $K(K+1)/2$ degrees of freedom. Therefore, @WHIT:80's\index[author]{White} test
can be viewed as a special case of @BREU:PAGA:79's test\index[author]{Breusch}\index[author]{Pagan}.

In his electric consumption regression, @HOUT:51\index[author]{Houthakker} used as covariates `inc` (average yearly income in pounds), the inverse of `mc6` (the marginal cost of electricity), `gas6` (the marginal price of gas) and `cap` (the average holdings of heavy electric equipment). The OLS estimation is:
\idxfun{lm}{stats}\idxdata[(]{uk\_elec}{micsr.data}

```{r}
#| label: lm_elec
lm_elec <- lm(kwh ~ inc + I(1 / mc6) +  gas6 + cap, uk_elec)
```

The $f$ vector in @eq-bp_heteros_ESS is:
\idxfun{resid}{stats}\idxfun{mean}{base}

```{r}
#| label: f_bp_heterosc
f <- 1 - resid(lm_elec) ^ 2 / mean(resid(lm_elec) ^ 2)
```

We then regress $f$ on $W$ (which is here a constant and the inverse of `cust`) and get half the explained sum of squares:
\idxfun{lm}{stats}\idxfun{sum}{base}\idxfun{fitted}{stats}

```{r}
#| label: bp_heterosc_1
lm_elec_resid <- lm(f ~ I(1 / cust), uk_elec)
bp1_elec <- sum(fitted(lm_elec_resid) ^ 2) / 2
```
For the second version of the test we compute $N$ times the $R ^ 2$:
\idxfun{nobs}{stats}\idxfun{rsq}{micsr}

```{r}
#| label: bp_heterosc_2
bp2_elec <- nobs(lm_elec_resid) * rsq(lm_elec_resid)
```

The values and the probability values for the two versions of the Breusch-Pagan test are:
\idxfun{pchisq}{stats}

```{r}
#| label: bp_heterosc_print
#| collapse: true
c(bp1_elec, bp2_elec)
pchisq(c(bp1_elec, bp2_elec), lower.tail = FALSE, df = 1)
```

The homoskedasticity hypothesis is therefore highly rejected.
The `lmtest::bptest` computes automatically the Breusch-Pagan test for heteroskedasticity, with two formulas: the first one being the formula of the model and the second one being a one-side formula for the skedasticity equation. An alternative syntax is to provide a `lm` model as the first argument:
\idxfun{bptest}{lmtest}

```{r}
#| label: lmtest_bptest
#| results: false
lmtest::bptest(kwh ~ inc + I(1 / mc6) + gas6 + cap, ~ I(1 / cust), 
               data = uk_elec, studentize = FALSE)
lmtest::bptest(lm_elec, ~ I(1 / cust),
               data = uk_elec, studentize = FALSE) %>% gaze
```

Note that we set the `studentize` argument to `FALSE`. The default value is `TRUE` and in this case, a modified version of the test due to @KOEN:81\index[author]{Koenker} is used.
\idxdata[)]{uk\_elec}{micsr.data}
\index[general]{test!heteroskedasticity|)}
\index[general]{Breusch Pagan test!heteroskedasticity|)}

### Testing for individual effects

\index[general]{test!individual effects|(}
\index[general]{Breusch Pagan test!individual effects|(}

@BREU:PAGA:80\index[author]{Breusch}\index[author]{Pagan} extend their Lagrange multiplier test for detecting
heteroskedasticity to the problem of individual (or entity) effects in a panel (or
in a pseudo-panel) setting. Assuming a normal distribution, the joint density for the whole sample is:

$$
f(y\mid X) = \frac{1}{(2\pi) ^ {NT /2}\mid\Omega\mid}e^{-\frac{1}{2}\epsilon ^ \top \Omega ^ {-1} \epsilon}
$$ {#eq-joint_density_panel}

We have seen (@eq-omega_panel) that $\Omega = \sigma_\nu ^ 2 W + \sigma_\iota ^ 2B$, with
$\sigma_\iota ^ 2 = \sigma_\nu + T \sigma_\eta$. Then, 

$$\epsilon^ \top \Omega ^ {-1} \epsilon = 
\frac{1}{\sigma_\nu ^ 2}\epsilon ^ \top W \epsilon + \frac{1}{\sigma_\iota ^ 2}\epsilon ^ \top B \epsilon$$
The determinant of $\Omega$ is the product of its eigen values, which are $\sigma_\nu ^ 2$ with periodicity $N(T-1)$ and $\sigma_\iota^2$ with periodicity $N$. Then, taking the logarithm of @eq-joint_density_panel and denoting $\theta^ \top = (\sigma_\nu ^ 2, \sigma_\eta ^ 2)$, we get the following log-likelihood function:

$$
\ln L (\theta)= \frac{NT}{2} \ln 2\pi - \frac{N(T-1)}{2}\ln \sigma_\nu ^ 2 - \frac{N}{2}\ln(T\sigma_\eta ^ 2 + \sigma_\nu ^ 2) - \frac{\epsilon ^ \top W \epsilon}{2\sigma_\nu ^ 2}- \frac{\epsilon ^ \top W \epsilon}{2(\sigma_\nu ^ 2 + T \sigma_\eta ^ 2)}
$$

The gradient and the hessian are respectively:

$$
g(\theta)=
\left(
  \begin{array}{cc}
    \frac{\partial \ln L}{\partial \sigma_\nu^2} \\ \frac{\partial
      \ln L}{\partial \sigma_\eta^2} \\
  \end{array}
  \right)
=
\left(
  \begin{array}{cc}
    -\frac{N(T-1)}{2\sigma_\nu^2}-\frac{N}{2\sigma_\iota^2}+
    \frac{\epsilon^\top W\epsilon}{2\sigma_\nu^4}+\frac{\epsilon^\top B_\eta\epsilon}{2\sigma_\iota^2}\\ 
    -\frac{NT}{2\sigma_\iota^2}+\frac{T
    \epsilon^\top B\epsilon}{2\sigma_\iota^2}
  \end{array}
\right)
$$

$$ H(\theta)= \left(
\begin{array}{ll}
  -\frac{N(T-1)}{2\sigma_\nu^4}+\frac{N}{2\sigma_\iota^4}-
  \frac{\epsilon^\top W\epsilon}{\sigma_\nu^6}-\frac{\epsilon^\top B\epsilon}{\sigma_\iota^6} 
  & \frac{NT}{2\sigma_\iota^4}-\frac{T\epsilon^\top B\epsilon}{\sigma_\iota^6}\\
  \frac{NT}{2\sigma_\iota^4}-
  \frac{T\epsilon^\top B\epsilon}{\sigma_\iota^6} 
  &\frac{NT^2}{2\sigma_\iota^4} -
    \frac{T^2 \epsilon^\top B\epsilon}{\sigma_\iota ^ 6}
\end{array}
\right)
$$

To compute the expectation of this matrix, we note that
$\mbox{E}(\epsilon^\top W_\eta\epsilon)=N(T-1)\sigma_\nu^2$ and
$\mbox{E}(\epsilon^\top B_\eta\epsilon)=N\sigma_\iota ^ 2$:

$$
\mbox{E}(H(\theta))= \left(
\begin{array}{cc}
  -\frac{N(T-1)}{2\sigma_\nu^4}-\frac{N}{2\sigma_\iota ^ 4} 
  & -\frac{NT}{2\sigma_\iota ^ 4}\\
  -\frac{NT}{2\sigma_\iota ^ 4} 
  & -\frac{NT^2}{2\sigma_\iota ^ 4}
\end{array}
\right)
$$

To compute the test statistic, we impose the null hypothesis:
$H_0: \sigma_\eta^2=0$ (no individual effects), so that $\sigma_\iota^2= \sigma_\nu ^ 2$. In this case,
the OLS estimator is BLUE and 
$\hat{\sigma}_\nu^2$ is $\hat{\epsilon}^\top\hat{\epsilon} / NT$. The
estimated score and the information matrix are, with $\hat{\theta}^\top = (\hat{\sigma}_\nu^ 2, 0)$

$$
\hat{g}(\hat{\theta})=
\left(
  \begin{array}{cc}
    0 \\ -\frac{NT}{2\hat{\sigma}_\nu^2}\left(\frac{\hat{\epsilon}^\top B\hat{\epsilon}}{N\hat{\sigma}_\nu^2}-1\right)
  \end{array}
\right)
$$

$$
\hat{I}(\hat{\theta}) = \mbox{E}\left(-H(\hat{\theta})\right)=
\frac{NT}{2\hat{\sigma}_\nu^4}
\left(
\begin{array}{cc}
  1 & 1 \\
  1 & T
\end{array}
\right)
$$

and the inverse of the estimated information matrix is:

$$
\hat{I}(\hat{\theta}) ^ {-1}=\frac{2\hat{\sigma}_\nu^4}{NT(T-1)}
\left(
\begin{array}{cc}
  T & -1 \\
  -1 & 1
\end{array}
\right)
$$

Finally, the test statistic is computed as the quadratic form: $\hat{g}(\hat{\theta}) ^ \top \hat{I}(\hat{\theta}) ^ {-1} \hat{g}(\hat{\theta})$ which simplifies to:

$$ LM = \left(-\frac{NT}{2\hat{\sigma}_\nu^2}
  \left(\frac{\hat{\epsilon}^\top B\hat{\epsilon}}{N\hat{\sigma}_\nu^2}-1\right)\right)^2
\times \frac{2\hat{\sigma}_\nu^4}{NT(T-1)} =
\frac{NT}{2(T-1)}\left(\frac{\hat{\epsilon}^\top B\hat{\epsilon}}{N\hat{\sigma}_\nu^2}-1\right)^2
$$ 

Or, replacing $\hat{\sigma}_\nu^2$ by
$\hat{\epsilon}^\top\hat{\epsilon}/NT$:

$$ LM = 
\frac{NT}{2(T-1)}\left(T\frac{\hat{\epsilon}^\top B\hat{\epsilon}}{\hat{\epsilon}^\top\hat{\epsilon}}-1\right)^2
$$ {#eq-bp_panel}

which is asymptotically distributed as a $\chi^2$ with 1 degree of
freedom.

\idxdata[(]{twins}{micsr.data}
@BONJ:CHERK:KASK:03\index[author]{Bonjour}\index[author]{Cherkas}\index[author]{Haskel}\index[author]{Hawkes}\index[author]{Spector} estimated a Mincer equation with a sample of twins, the entity index being `family`. The response is the log of wage and the covariates are education (`educ`) and potential experience and its square (approximated by the age `age`):
\idxfun{lm}{stats}\idxfun{coef}{stats}

```{r}
#| label: twins_lm 
#| collapse: true
lm_twins <- lm(log(earning) ~ poly(age, 2) + educ, twins)
lm_twins %>% coef
```

We then add the residuals to the data an we compute the individual mean of the residuals by grouping by entities and then using `mutate` and not `summarise` to compute the mean,  $B\epsilon$ being a vector of length $N \times T$ where each value is returned $T$ times.
\idxfun{add\_column}{tibble}\idxfun{resid}{stats}\idxfun{group\_by}{dplyr}\idxfun{ungroup}{dplyr}\idxfun{mean}{base}

```{r}
#| label: twins_resid_Bresid
twins <- twins %>% add_column(e = resid(lm_twins)) %>% 
  group_by(family) %>% 
  mutate(Be = mean(e)) %>% ungroup
```

We finally compute the statistic using @eq-bp_panel:
\idxfun{pull}{dplyr}\idxfun{unique}{base}\idxfun{length}{base}\idxfun{sum}{base}\idxfun{summarise}{dplyr}

```{r}
#| label: bp_twins_manual
#| collapse: true
N_tw <- twins %>% pull(family) %>% unique %>% length
T_tw <- 2
twins %>% 
  summarise(bp = 0.5 * T_tw * N_tw / (T_tw - 1)  * 
              (T_tw * sum(Be ^ 2) / sum(e ^ 2) - 1) ^ 2) %>% pull
```


The **plm** package [@CROI:MILL:08; @CROI:MILL:18]\index[author]{Croissant}\index[author]{Millo} provides different tools to deal with panel or pseudo-panel data. In particular, @BREU:PAGA:80's\index[author]{Breusch}\index[author]{Pagan} test can easily be obtained using `plm::plmtest`. We set the `type` argument to `"bp"` to get the statistic of the original Breusch-Pagan test:
\idxfun{plmtest}{plm}\idxfun{gaze}{micsr}

```{r}
#| label: bp_twins_plmtest
#| collapse: true
library(plm)
plmtest(log(earning) ~ poly(age, 2) + educ, twins, type = "bp") %>% gaze
```

\idxdata[)]{twins}{micsr.data}
The absence of individual effects is rejected at the 5% level, but not at the 1% level.

The model fitted by @SCHA:90\index[author]{Schaller} is a simple linear model, the response being the rate of investment (`ikn`)  and the unique covariate Tobin's Q (`qn`):
\idxfun{print}{base}\idxdata[(]{tobinq}{micsr.data}

```{r}
tobinq %>% print(n = 3)
```

The first two columns contain the firm and the time index. The Breusch-Pagan statistic is:
\idxfun{plmtest}{plm}\idxfun{gaze}{micsr}

```{r}
#| collapse: true
plmtest(ikn ~ qn, tobinq, type = "bp") %>% gaze
```

The statistic is huge, the hypothesis of no individual effects is therefore very strongly rejected, which is a quite customary result for panel data, especially when the time dimension is high, which is the case for the `tobinq` data (35 years).
\idxdata[)]{tobinq}{micsr.data}

\index[general]{test!individual effects|)}
\index[general]{Breusch Pagan test!individual effects|)}

### System of equations {#sec-bptest_system}

\index[general]{test!inter-equation correlation|(}
\index[general]{Breusch Pagan test!inter-equation correlation|(}


```{r}
#| label: repeat_syslm
#| echo: false
ap <- apples %>% filter(year == 1985) %>%
    transmute(y = otherprod + apples,
              ct = capital + labor + materials,
              sl = labor / ct, sm = materials / ct,
              pk = log(pc / mean(pc)), pl = log(pl / mean(pl)) - pk,
              pm = log(pm / mean(pm)) - pk, ct = log(ct / mean(ct)) - pk,
              y = log(y / mean(y)), y2 = 0.5 * y ^ 2,
              ct = ct, pl2 = 0.5 * pl ^ 2,
              pm2 = 0.5 * pm ^ 2, plm = pl * pm
              )
eq_ct <- ct ~ y + y2 + pl + pm + pl2 + plm + pm2
eq_sl <- sl ~ pl + pm
eq_sm <- sm ~ pl + pm
library(Formula)
eq_sys <- Formula(ct + sl + sm ~ y + y2 + pl + pm + pl2 + plm + pm2)
mf <- model.frame(eq_sys, ap)  ; Z_c <- model.matrix(eq_ct, mf) 
Z_l <- model.matrix(eq_sl, mf) ; Z_m <- model.matrix(eq_sm, mf)
nms_cols <- function(x, label)
    paste(label, c("cst", colnames(x)[-1]), sep = "_")
nms_c <- nms_cols(Z_c, "cost") ; nms_l <- nms_cols(Z_l, "sl")
nms_m <- nms_cols(Z_m, "sm")
Zs <- Matrix::bdiag(Z_c, Z_l, Z_m) %>% as.matrix
colnames(Zs) <- c(nms_c, nms_l, nms_m)
Y <- model.part(eq_sys, mf, rhs = 0, lhs = 1)
ys <- Y %>% pivot_longer(1:3, cols_vary = "slowest", names_to = "equation",
                         values_to = "response")
stack_data <- ys %>% bind_cols(Zs)
ols_unconst <- lm(response ~ . - 1 - equation, stack_data)
R <- matrix(0, nrow = 6, ncol = 14)
R[1, c(4,  9)] <- R[2, c(5, 12)] <- R[3, c(6, 10)] <- R[4, c(7, 11)] <-
    R[5, c(7, 13)] <- R[6, c(8, 14)] <- c(1, -1)
ols_const <- clm(ols_unconst, R)
```

@BREU:PAGA:80\index[author]{Breusch}\index[author]{Pagan} also proposed a test for the absence of correlation between equations in a system of equation. Reminds that in this case, the covariance matrix of the errors for the whole system is $\Omega = \Sigma \otimes I$, where $\Sigma$ contains the variances (on the diagonal) and the covariances (off diagonal) of the errors of the $L$ equations. This symmetric matrix contains $L \times (L + 1) / 2$ distinct elements, $L \times (L + 1) / 2 - L = L \times (L - 1) / 2$ being covariances. The Breusch-Pagan test is based on the estimation of the the covariance matrix using OLS residuals. Denoting $\hat{\Xi} = (\hat{\epsilon}_1, \hat{\epsilon}_2, \ldots \hat{\epsilon}_L)$ the matrix where each column contains the vector of residuals for one equation:

$$
\hat{\Omega} = \hat{\Xi} ^ \top \hat{\Xi}=
\left(
\begin{array}{cccc}
\hat{\epsilon}_1 ^ \top \hat{\epsilon_1} &\hat{\epsilon}_1 ^ \top \hat{\epsilon_2} & \ldots &\hat{\epsilon}_1 ^ \top \hat{\epsilon_L} \\
\hat{\epsilon}_2 ^ \top \hat{\epsilon_1} &\hat{\epsilon}_2 ^ \top \hat{\epsilon_2} & \ldots &\hat{\epsilon}_2 ^ \top \hat{\epsilon_L} \\
\vdots & \vdots & \ddots & \vdots \\
\hat{\epsilon}_L ^ \top \hat{\epsilon_1} &\hat{\epsilon}_L ^ \top \hat{\epsilon_2} & \ldots &\hat{\epsilon}_L ^ \top \hat{\epsilon_L}
\end{array}
\right)
$$
The coefficients of correlations are then estimated, denoting $\hat{\sigma} ^ 2_l = \hat{\epsilon}_l^\top \hat{\epsilon}_l/ N$ the estimated covariance:

$$
\hat{\rho}_{lm} = \frac{\hat{\epsilon}_l^\top \hat{\epsilon}_m / N}{\hat{\sigma}_l\hat{\sigma}_m}
$$
The statistic is then $N \sum_{l=1} ^ {L-1} \sum_{m = l + 1} ^ L \hat{\rho}_{lm}^2$ and is a $\chi^2$ with $L(L-1) / 2$ degrees of freedom if the hypothesis of no correlation is true.

We use the apple production example estimated in @sec-system_equation. The estimation for the whole system by OLS taking constrains into account was stored in an object called `ols_const`. We extract the residuals and arrange them in a $N\times L$ matrix. Taking the cross-product of this matrix and dividing by $N$, we get $\hat{\Sigma}$:
\idxfun{nobs}{stats}\idxfun{resid}{stats}\idxfun{matrix}{base}\idxfun{crossprod}{base}

```{r }
#| label: est_Sigma
N_ap <- nobs(ols_const) / 3
EPS <- ols_const %>% resid %>% matrix(ncol = 3)
Sigma <- crossprod(EPS) / N_ap
Sigma
```

We then compute the estimated standard deviation of the errors for every equation $\hat{\sigma}_l$ and we divide $\hat{\Sigma}$ by a matrix containing the products of the standard deviations, using the `outer` function:
\idxfun{stder}{micsr}\idxfun{outer}{base}

```{r }
#| label: est_corr_matrix
sig <- Sigma %>% stder
d <- Sigma / outer(sig, sig)
d
```

so that we get a matrix with ones on the diagonal and coefficients of
correlations off-diagonal. Then, we extract the off-diagonal elements
using `upper.tri`, which returns a logical matrix with values of `TRUE` above the diagonal:
\idxfun{upper.tri}{base}

```{r}
upper.tri(d)
```

Then, indexing `d` by `upper.tri(d)` returns a vector containing the three elements of the matrix that are above the diagonal:^[As the matrix is symmetric, `lower.tri` could also have been used.]
\idxfun{upper.tri}{base}

```{r}
#| collapse: true
d[upper.tri(d)]
```

Finally, we sum the squares of the elements of this vector and multiply by the sample size to get the statistic:
\idxfun{sum}{base}\idxfun{upper.tri}{base}\idxfun{pchisq}{stats}

```{r}
#| label: bptest_panel
#| collapse: true
bp_stat <- sum( d[upper.tri(d)] ^ 2) * N_ap
pval_pb <- pchisq(bp_stat, df = 3, lower.tail = FALSE)
c(bp_stat, pval_pb)
```
The hypothesis of no correlation is clearly rejected.
\index[general]{test!inter-equation correlation|)}
\index[general]{Breusch Pagan test!inter-equation correlation|)}


## Robust inference {#sec-sandwich}

\index[general]{sandwich|(}
\index[general]{covariance matrix estimation!sandwich|(}

If the errors are not spherical, the simple estimator of the covariance
of the OLS estimates is biased. More general estimators can then be used
instead. These estimators use the residuals of the OLS estimator which,
in the context of this chapter, is an inefficient but consistent
estimator. We'll present the robust estimator of the covariance matrix of the OLS estimates first in the context of
the simple linear model and then for the multiple linear model.

### Simple linear model

\index[general]{covariance matrix estimation!simple linear regression!sandwich|(}
The variance of the slope estimated by OLS is:

$$
\sigma_{\hat{\beta}} ^ 2 = 
\mbox{V}(\hat{\beta} \mid x) = \frac{\mbox{E}\left(\left[\sum_n (x_n - \bar{x})\epsilon_n\right]^2\mid x\right)}{\left(\sum_n (x_n - \bar{x}) ^ 2\right) ^ 2}
$$

The numerator is the sum of the expectations of $N ^ 2$ terms. For
$N = 4$, replacing the errors $\epsilon_n$ by the OLS residuals
$\hat{\epsilon}_n$ and dropping the expectation operator, these 16 terms
can be presented conveniently in the following matrix:

```{r }
#| label: robust_simple_linear_general
#| echo: false
#| results: "asis"
cat("$$\\small{\n\\left(\n\\begin{array}{cccc}\n")
strgr <- c()
for (i in 1:4){
    for (j in 1:4){
        if (i == j){
            strgr <- c(strgr, paste("(x_", i, "-\\bar{x})^2 \\hat{\\epsilon}_", i, "^2", sep = ""))
        }
        else
            strgr <- c(strgr, paste("(x_", i, "-\\bar{x}) (x_", j, "-\\bar{x}) \\hat{\\epsilon}_", i, "\\hat{\\epsilon}_", j,sep = ""))
        strgr <- c(strgr, ifelse(j == 4, " \\\\ \n", " & \n"))
    }
}
strgr <- paste(strgr, collapse = "")
cat(strgr)
cat("\\end{array}\n\\right)}$$")
```

The robust estimator is obtained by taking the sum of *some* of this
terms. Note first that the sum of all these terms is
$\left(\sum_{n = 1} ^ N (x_n - \bar{x})\hat{\epsilon}_{n}\right)^2$,
which is equal to zero as:
$\sum_{n = 1} ^ N (x_n - \bar{x})\hat{\epsilon}_n=0$. Therefore, it is
not relevant to sum *all* the terms of this matrix to get an estimator
of the variance of $\hat{\beta}$.
The first possibility is to take only the diagonal terms of this matrix,
which is relevant if we maintain the hypotheses that the errors are
uncorrelated. In this case, we get the so-called
**heterosckedastic-consistent** (or **HC**) estimator of
$\sigma_{\hat{\beta}}$ proposed by @WHIT:80:\index[author]{White}
\index[general]{sandwich!heteroskedastic-consistent}

$$
\hat{\sigma}_{\mbox{HC}\hat{\beta}} ^ 2 = \frac{1}{S_{xx} ^ 2}\sum_{n = 1}
^ N (x_n - \bar{x}) ^ 2 \hat{\epsilon}_n ^ 2 
$$ {#eq-vcovHC}

Consider now the case where some errors are correlated This often
happens when some observations share some common unobserved
characteristics which are included in their (therefore correlated)
errors. For example, if observations belong to different regions, their
errors may share some common unobserved features of the regions.
In our 4 observations case, suppose that the first two
observations belong to one group, and the two other to an other group.
Then, a consistent estimator is obtained by summing the
following subset of elements of the preceding matrix:
\index[general]{sandwich!cluster|(}

```{r }
#| echo: false
#| results: asis
#| label: robust_simple_linear_cluster
cat("$$\\small{\\left(\\begin{array}{cccc}")
strgr <- c()
for (i in 1:4){
    for (j in 1:4){
        if (i == j){
            strgr <- c(strgr, paste("(x_", i, "-\\bar{x})^2 \\hat{\\epsilon}_", i, "^2", sep = ""))
        }
        else{
            if ( ( (i %in% 1:2) & (j %in% 1:2) ) | ( (i %in% 3:4) & (j %in% 3:4) ))
                strgr <- c(strgr, paste("(x_", i, "-\\bar{x}) (x_", j, "-\\bar{x}) \\hat{\\epsilon}_",
                                        i, "\\hat{\\epsilon}_", j,sep = ""))
            else strgr <- c(strgr, "\\mbox{--}")
        }
        strgr <- c(strgr, ifelse(j == 4, " \\\\ \n", " & \n"))
    }
}
strgr <- paste(strgr, collapse = "")
cat(strgr)
cat("\\end{array}\\right)}$$\n")
```

which leads to the clustered estimated variance. More generally, for $N$
observations belonging to $G$ groups, this estimator is:

$$
\hat{\sigma}_{\mbox{CL}\hat{\beta}} ^2= \frac{1}{S_{xx}^2}\sum_{g = 1} ^ G \left(\sum_{n \in g} (x_n -
\bar{x})\hat{\epsilon}_n \right) ^ 2
$$ {#eq-vcovCL}

which is consistent with the hypothesis that errors are correlated
*within* a group, but uncorrelated *between* group.
\index[general]{sandwich!cluster|)}
To illustrate the computation of robust covariance estimators, we use
the data set `urban_gradient` of @DURA:PUGA:20\index[author]{Duranton}\index[author]{Puga}. It contains, the
population, the area and the distance to the central business district
for 2315 block groups in Alabama.^[Actually, the whole data set covers the whole United States, but we use here the small subsample that concerns the state of Alabama.]

```{r}
#| label: urban_gradient
urban_gradient %>% head
```

A classic model in urban economics states that urban density is a
negative exponential function of the distance to the central business
district : $y = A e^{\beta x}$ where $y$ is measured in inhabitants per
square kilometers, $x$ is measured in kilometers and $\beta<0$ is
called the urban gradient. Taking logs, this leads to a semi-log linear
regression model:

$$
\ln y_n = \alpha + \beta x_n + \epsilon_n
$$

We first compute the `density` variable and then estimate the urban
gradient model.
\idxdata[(]{urban\_gradient}{micsr.data}
\idxfun{mutate}{dplyr}\idxfun{lm}{stats}\idxfun{gaze}{micsr}

```{r}
#| label: urban_gradient_lm
#| collapse: true
urban_gradient <- urban_gradient %>% mutate(density = population / area)
ols_ug <- lm(log(density) ~ distance, urban_gradient)
ols_ug %>% gaze
```

The estimated standard deviation of the slope is
`r round(sqrt(vcov(ols_ug)[2, 2]), 4)` but it may be seriously biased if the
errors are heteroskedastic and/or correlated. We first plot the data
and the regression line on @fig-dataug, the shape of the points
depending on the metropolitan statistical area (MSA in short, there are 12 of them in
the sample).
\idxfun{ggplot}{ggplot2}\idxfun{aes}{ggplot2}\idxfun{geom\_smooth}{ggplot2}\idxfun{scale\_shape\_manual}{ggplot2}

```{r}
#| label: fig-dataug
#| fig-cap: "Data and regression line for the `urban_gradient` data set"
urban_gradient %>% ggplot(aes(distance, log(density))) + 
  geom_point(aes(shape = msa), size = .3) + 
  geom_smooth(method = "lm", se = FALSE, color = "black") +
  scale_shape_manual(values = c(3, 16, 17, 8, 5, 2, 1, 4, 6, 9, 0, 12))
```

Heteroskedasticity seems to be present in this data set, as the size of
the residuals seems to be an increasing function of the unique
covariate. We first compute the HC standard deviation of the slope, computing the mean of the covariate and $S_{xx}$ and then applying @eq-vcovHC:
\idxfun{pull}{dplyr}\idxfun{mean}{base}\idxfun{sum}{base}\idxfun{sqrt}{base}\idxfun{resid}{stats}\idxfun{summarise}{dplyr}

```{r}
#| label: hc_urban_gradient
#| collapse: true
dist_mean <- urban_gradient %>% pull(distance) %>% mean
Sxx <- sum( (pull(urban_gradient, distance) - dist_mean) ^ 2)
sd_hc <- urban_gradient %>% 
  summarise( sqrt(sum( (distance - dist_mean) ^ 2 * 
                         resid(ols_ug) ^ 2) / Sxx ^ 2)) %>% 
  pull
sd_hc
```

In this example, heteroskedastic-robust standard error is just slightly than the one computed using the simple formula. However, we also have to investigate the potential correlation
between the errors of some observations.
There are 12 MSA and 22 counties in Alabama. It is possible that 
errors for block groups of the same county or of the same MSA
are correlated (because of some unobserved common features of block
groups in the same county or MSA). In this case the covariance matrix of
the OLS estimates would be biased. We compute the estimation of the
clustered standard deviation of the slope (@eq-vcovCL) at the MSA level:
\idxfun{pull}{dplyr}\idxfun{unique}{base}\idxfun{length}{base}\idxfun{add\_column}{tibble}\idxfun{group\_by}{dplyr}\idxfun{resid}{stats}\idxfun{summarise}{dplyr}\idxfun{sum}{base}\idxfun{sqrt}{base}

```{r }
#| label: cl_urban_gradient
#| collapse: true
G <- urban_gradient %>% pull(msa) %>% unique %>% length
sd_CL <- urban_gradient %>% 
  add_column(eps = resid(ols_ug)) %>% 
  group_by(msa) %>% 
  summarise(z = sum( (distance - dist_mean) * eps) ^ 2) %>% 
  summarise(sd = sqrt(sum(z)) / Sxx) %>% 
  pull
sd_CL
```
\idxdata[)]{urban\_gradient}{micsr.data}

This time, we get a much higher estimate of the standard deviation of the slope
(about three time larger than the one obtained with the simple formula).

\index[general]{covariance matrix estimation!simple linear regression!sandwich|)}
### Multiple linear model

\index[general]{covariance matrix estimation!multiple linear regression!sandwich|)}
Consider now the multiple regression model. The vector of slopes can be
written as a linear combination of the vector of response, and then of
the vector of error:

$$
\begin{array}{rcl}
\hat{\beta}&=&(\tilde{X} ^ \top \tilde{X})^{-1}\tilde{X} ^ \top \tilde{y} \\
&=&(\tilde{X} ^ \top \tilde{X})^{-1}\tilde{X} ^ \top(\tilde{X}\beta+\epsilon)\\
&=&\beta+(\tilde{X} ^ \top \tilde{X})^{-1}\tilde{X} ^ \top\epsilon \\
\end{array}
$$

$\tilde{X} ^ \top\epsilon$ is a $K$-length vector containing the product
of the covariates (the column of $X$) in deviation from their sample
mean and the vector of errors:

$$
\tilde{X} ^ \top\epsilon =
\left(
\begin{array}{c}
\sum_{n=1} ^ N (x_{1n} - \bar{x}_1) \epsilon_n \\
\sum_{n=1} ^ N (x_{2n} - \bar{x}_2) \epsilon_n \\
\vdots \\
\sum_{n=1} ^ N (x_{1n} - \bar{x}_K) \epsilon_n
\end{array}
\right) 
= \sum_{n = 1} ^ N \psi_n
$$ 

$\psi_n$ is called the vector of score, as it is proportional to the
vector of the first derivatives of the sum of square residuals and is
therefore equal to 0 when evaluated for $\hat{\beta}$, the OLS
estimator. The general form of the covariance of the OLS estimates was
given in @eq-general_variance:

$$
\hat{V}(\hat{\beta}) = \frac{1}{N}
\left(\frac{1}{N}
\tilde{X} ^ \top \tilde{X}\right)^{-1}\frac{1}{N}\mbox{E}(\tilde{X} ^ \top \epsilon \epsilon ^
\top \tilde{X} \mid X) \left(\frac{1}{N}\tilde{X} ^ \top \tilde{X}\right) ^ {-1}
$$

This is a sandwich formula, the meat (the variance of the score) being surrounded by two slices of bread (the inverse of the covariance matrix of the covariates).
Remind from @sec-variance_ols that the meat can be written, for $K = 2$, as the expected value of:

$$
\scriptsize
{
\frac{1}{N}
\left(
\begin{array}{cccc}
\left(\sum_{n=1} ^ N (x_{1n} - \bar{x}_1) \epsilon_n\right) ^ 2 &
\left(\sum_{n=1} ^ N (x_{1n} - \bar{x}_1) \epsilon_n\right) 
\left(\sum_{n=1} ^ N (x_{2n} - \bar{x}_2) \epsilon_n\right)  \\
\left(\sum_{n=1} ^ N (x_{1n} - \bar{x}_1) \epsilon_n\right) 
\left(\sum_{n=1} ^ N (x_{2n} - \bar{x}_2) \epsilon_n\right) &
\left(\sum_{n=1} ^ N (x_{2n} - \bar{x}_2) \epsilon_n\right) ^ 2
\end{array}
\right)
}
$$

If the errors are uncorrelated, but potentially heteroskedastic, we use
the following estimation:
\index[general]{sandwich!heteroskedastic-consistent|(}

$$
\frac{1}{N}
\sum_{n = 1} ^ N
 \hat{\epsilon}_n ^ 2
\left(
\begin{array}{cccc}
 (x_{1n} - \bar{x}_1) ^ 2 &
 (x_{1n} - \bar{x}_1) (x_{2n} - \bar{x}_2) \\
 (x_{1n} - \bar{x}_1) (x_{2n} - \bar{x}_2) & 
 (x_{2n} - \bar{x}_2) ^ 2 \\
\end{array}
\right)
$$

which generalize the scalar case of the heteroskedastic covariance
matrix computed for the single regression case. This estimator of the variance of the score can easily be obtained by defining the **estimating function** $F$ which is obtained by multiplying (elements by elements) the columns of the matrix of covariates (in deviation from the sample means) by the vector of residuals:
\index[general]{estimating function}

$$
F =
\left(
\begin{array}{ccc}
\hat{\epsilon}_1 x_{11} & \ldots & \hat{\epsilon}_1 \tilde{x}_{1K} \\
\hat{\epsilon}_2 x_{21} & \ldots & \hat{\epsilon}_2 \tilde{x}_{2K} \\
\vdots & \ddots & \vdots\\
\hat{\epsilon}_N x_{N1} & \ldots & \hat{\epsilon}_N \tilde{x}_{NK}
\end{array}
\right)
$$
Then $F ^ \top F = \sum_{n=1} ^ N \hat{\epsilon}_n ^ 2 \tilde{x}_n \tilde{x}_n ^ \top$ is $N$ times the HC estimator of the meat and the HC estimator is then:

$$
\hat{V}(\hat{\beta}) = \frac{1}{N}\left(\frac{1}{N}\tilde{X} ^ \top \tilde{X}\right) ^ {-1}
\left(\frac{1}{N}\sum_{n=1} ^ N \hat{\epsilon}_n ^ 2 \tilde{x}_n \tilde{x}_n ^ \top\right)
\left(\frac{1}{N}\tilde{X} ^ \top \tilde{X}\right) ^ {-1}
$$ {#eq-sandwich_hc_multi}
\index[general]{sandwich!heteroskedastic-consistent|)}
\index[general]{sandwich!cluster|(}
To get the clustered estimator of the variance, we define for each
cluster $\psi_g = \sum_{n \in g} \psi_n$, and the clustered expression of the meat is obtained
as the sum of the outer products of $\psi_g$ divided by $N$:

$$
\frac{1}{N} \sum_{g=1} ^ G \hat{\psi}_g \hat{\psi}_g ^ \top
$$ {#eq-clustered_meat}

\idxdata[(]{uk\_elec}{micsr.data}
Going back to the electricity consumption estimation, `lm_elec` is the
model fitted by OLS. We first compute the estimator manually,
computing the meat (the cross products of the model matrix multiplied
by the vector of residuals divided by the sample size) and the bread
(the inverse of the cross products of the model matrix divided by the sample size). The covariance matrix is then computed using @eq-sandwich_hc_multi and we extract the standard deviations of the estimates, ie the square root of the diagonal elements of this matrix:
\idxfun{nobs}{stats}\idxfun{model.matrix}{stats}\idxfun{resid}{stats}\idxfun{crossprod}{base}\idxfun{solve}{base}

```{r}
#| label: vcov_hc_multi_manual
#| collapse: true
N_el <- nobs(lm_elec)
Z <- model.matrix(lm_elec)
M <- crossprod(resid(lm_elec) * Z) / N_el
B <- solve(crossprod(Z) / N_el)
(B %*% M %*% B / N_el) %>% stder
```

\index[general]{sandwich!cluster|(}
The **sandwich** package [@ZEIL:04; @ZEIL:06; @ZEIL:KOLL:NATH:20.]\index[author]{Zeileis}\index[author]{KÃ¶ll}\index[author]{Graham}, provides specialized functions to construct the different pieces of the estimator, namely `estfun`, `meat` and `bread`:
\idxfun{estfun}{sandwich}\idxfun{meat}{sandwich}\idxfun{bread}{sandwich}\idxfun{stder}{micsr}

```{r}
#| label: vcov_hc_multi_pieces
#| collapse: true
library(sandwich)
F <- estfun(lm_elec)
M <- meat(lm_elec)
B <- bread(lm_elec)
(B %*% M %*% B / N_el) %>% stder
```

`sandwich::vcovHC` uses these functions to compute the HC estimator; it's first
argument is a fitted model and a `type` argument can also be supplied. Setting `type` to `"HC0"` gives the simplest version of the estimator, the one we have
previously computed "by hand". Other flavors of this
heteroskedasciticy-robust estimator can be obtained by setting the
`type` to `"HC1"`, $\ldots$, `"HC5"` to perform different kinds of degrees of
freedom correction. For example, when `type = "HC1"` (which is the default), the covariance
matrix is multiplied by $N / (N - K)$:[^non_spherical-2]

[^non_spherical-2]: See @ZEIL:06\index[author]{Zeileis} for more details concerning the other
    values of the `type` argument.
\idxfun{vcovHC}{sandwich}

```{r}
#| label: vcov_hc_multi_auto
#| collapse: true
vcovHC(lm_elec, type = "HC0") %>% stder
```

Comparing the standard and the robust estimation of the standard deviations of the estimates, we get:
\idxfun{vcovHC}{sandwich}\idxfun{vcov}{stats}\idxfun{stder}{micsr}\idxfun{rbind}{base}

```{r}
#| label: robust_standard_sd
robust_sd <- vcovHC(lm_elec, type = "HC0") %>% stder
standard_sd <- vcov(lm_elec) %>% stder
rbind(standard = standard_sd, robust = robust_sd, 
      ratio = robust_sd / standard_sd)
```

\idxdata[)]{uk\_elec}{micsr.data}
In this example, the robust standard errors are very close to the ones obtained using the simple formula, although the Breusch-Pagan test rejected the hypothesis of homoskedasticity. This is because the source of heteroskedasticity (the number of customers) is not a covariate of the model.

\idxdata[(]{twins}{micsr.data}
To illustrate the use of the clustered sandwich estimator, we use the `twins` data set. `lm_twins` is the OLS estimation of the Mincer equation. In this context, the meat can easily be computed using the `apply` and `tapply` functions:

- `apply` performs an operation on one of the margin (1 for rows, 2 for columns) of a matrix,
- `tapply` performs an operation (the third argument) on a vector conditional (the first argument) on the values of an other vector (the second argument).
\idxfun{model.matrix}{stats}\idxfun{resid}{stats}\idxfun{crossprod}{base}\idxfun{tapply}{base}\idxfun{apply}{base}\idxfun{solve}{base}

```{r}
Z <- model.matrix(lm_twins)
e <- resid(lm_twins)
id <- twins$family
Ze <- Z * e
M <- crossprod(apply(Ze, 2, tapply, id, sum)) / N_tw
B <- solve(crossprod(Z) / N_tw)
V_1 <- (B %*% M %*% B / N_tw)
```

We can more simply use the `sandwich::vcovCL` function [@ZEIL:KOLL:NATH:20]\index[author]{Zeileis}\index[author]{KÃ¶ll}\index[author]{Graham} to compute the clustered estimator. The clustering variable is defined using the `cluster` argument
that can be set to a one-sided formula. The `type` argument is similar
to the one of `vcovHC` and there is also a `cadjust` argument which, if
`TRUE`, multiplies the covariance matrix by $G / (G - 1)$, $G$ being the
number of clusters. The default behavior of `vcovCL` is to set `cadjust` to `TRUE` and
`type` to `"HC1"`, so that the adjustment is done for the number of
observations and for the number of groups.
For the `twins` data set, the clustering variable is `family`:
\idxfun{vcovCL}{sandwich}

```{r}
V_2 <- vcovCL(lm_twins, ~ family, type = "HC0", cadjust = FALSE)
```

Finally, `twins` being a pseudo-panel, the `plm` package can also be used. To OLS estimate can be fitted with `plm::plm` by setting the `model` argument to `"pooling"`.
\idxfun{plm}{plm}

```{r}
plm_twins <- plm(log(earning) ~ poly(age, 2) + educ, twins, 
                 model = "pooling")
```

A `plm` object is returned and the `vcovHC` method for `plm`s objects by default returns the same clustered covariance matrix as previously:
\idxfun{vcovHC}{sandwich}

```{r}
V_3 <- vcovHC(plm_twins)
```

The `vcovHC` and `vcovCL` functions are particularly useful while using the testing functions of the **lmtest** package. For example, `lmtest::coeftest` computes the usual table of coefficients (the same as the one obtained using `summary`), but a matrix or a function that computes a covariance matrix can be passed as a supplementary argument. Therefore, `lmtest::coeftest(lm_twins)` returns exactly the same table of coefficients as `summary(lm_twins)`, but other standard errors are obtained by filling the second argument. For example, to get the clustered standard errors with our preferred specification without degrees of freedom correction (`type = "HC0", cadjust = FALSE`), we can use any of the following three equivalent syntax:
\idxfun{vcovCL}{sandwich}\idxfun{coeftest}{lmtest}\idxfun{function}{base}

```{r}
#| results: false
cl <- vcovCL(lm_twins, ~ family, cadjust = FALSE, type = "HC0")
library(lmtest)
coeftest(lm_twins, cl)
coeftest(lm_twins, function(x) 
  vcovCL(x, ~ family, cadjust = FALSE, type = "HC0"))
coeftest(lm_twins, vcovCL, cluster = ~ family, 
         cadjust = FALSE, type = "HC0")
```

In the first expression, we provide a matrix previously computed. In the second expression, we use an anonymous function with our preferred options. In the last one, the two arguments of `vcovCL` are indicated as arguments of `coeftest` and are passed internally to `vcovCL`.
We finally compare the ordinary and the robust estimates of the standard deviations of the OLS estimate:
\idxfun{vcov}{stats}\idxfun{stder}{micsr}\idxfun{rbind}{base}

```{r}
ord_se <- vcov(lm_twins) %>% stder
cl_se <- V_1 %>% stder
rbind(ordinary = ord_se, cluster = cl_se, 
ratio = cl_se / ord_se)
```

As for the electricity consumption example, we can see that the robust standard errors are almost the same as those computed using the simple formula.\idxdata[)]{twins}{micsr.data}

<!-- Finally, for our panel data example, we get: -->

<!-- ```{r} -->
<!-- ols_q <- plm(ikn ~ qn, tobinq, model = "pooling") -->
<!-- ord_sd <- vcov(ols_q) %>% stder -->
<!-- cl_sd <- vcovHC(ols_q) %>% stder -->
<!-- rbind(ordinary = ord_sd, cluster = cl_sd,  -->
<!-- ratio = cl_sd / ord_sd) -->
<!-- ``` -->

<!-- and in this case the robust standard error is much larger (about 4 times) than the simple one. -->

\index[general]{covariance matrix estimation!multiple linear regression!sandwich|)}
\index[general]{sandwich|)}
\index[general]{covariance matrix estimation!sandwich|)}


## Generalized least squares estimator {#sec-gls}
\index[general]{generalized least squares|(}

For a linear model
$y_n = \alpha + \beta^\top x_n + \epsilon_n = \gamma ^ \top z_n + \epsilon_n$
with non-spherical disturbances ($\Omega \neq \sigma_\epsilon ^ 2 I$),
the OLS estimator is now longer BLUE. A more efficient estimator called
the **generalized least squares** (**GLS** in short) can then be used instead.

### General formulation of the GLS estimator

The GLS estimator is:

$$
\hat{\gamma} = (Z^\top\Omega^{-1}X) ^ {-1} Z^\top \Omega ^ {-1} y
$$ {#eq-glsmatrix}

Replacing $y$ by
$Z\gamma+\epsilon$, we get:

$$
\hat{\gamma}=(Z^\top\Omega^{-1}Z) ^ {-1} Z^\top \Omega ^ {-1} (Z\gamma + \epsilon) = \gamma + (Z^\top\Omega^{-1}Z) ^ {-1} Z^\top \Omega ^ {-1} \epsilon
$$ As for the OLS estimator, the estimator is unbiased if
$\mbox{E}\left(\epsilon\mid Z\right)=0$. The variance is:

$$
\begin{array}{rcl}
\mbox{V}(\hat{\gamma}) &=& \mbox{E}\left[(Z^\top\Omega^{-1}Z) ^ {-1} Z^\top \Omega ^ {-1} \epsilon\epsilon^\top \Omega^{-1}Z(Z^\top \Omega ^ {-1} Z)^{-1}\right]\\
&=& (Z^\top\Omega^{-1}Z) ^ {-1} Z^\top \Omega ^ {-1} \mbox{E}(\epsilon\epsilon ^ \top) \Omega Z (Z^\top\Omega^{-1}Z) ^ {-1}\\
&=&(Z^\top\Omega^{-1}Z)^{-1}
\end{array}
$$

Written this way, the GLS estimator is unfeasible for two reasons:

-   the $\Omega$ matrix is a square matrix of dimension $N\times N$, and
    it is computationally difficult (or impossible) to store it and to
    invert for a large sample,
-   it uses a matrix $\Omega$ which contains $N(N+1) / 2$ unknown
    parameters.

A **feasible GLS** estimator is obtained by imposing some structure on
$\Omega$ so that the number of unknown parameters became much less than
$N(N+1)/2$ and by estimating these unknown parameters using residuals of
a first step consistent estimation. Actually, in practice, the GLS
estimator is obtained by performing OLS on transformed data. More
precisely, consider the matrix $C$ such that
$C ^ \top C = \Omega ^ {-1}$. Then, @eq-glsmatrix can be rewritten,
denoting $w^*=Cw$:

$$
\hat{\gamma} = (Z^\top C^\top CZ) ^ {-1} Z^\top C^\top C y = \left(Z^{*\top}Z^*\right)^{-1} Z^{*\top}y^*
$$ {#eq-glstrans}

which is the OLS estimator of the linear model:
$y^* = Z^*\gamma + \epsilon^*$, with $\epsilon^* = C\epsilon$. Replacing
$y^*$ by in $Z^*\gamma + \epsilon^*$, @eq-glstrans, we get:

$$
\hat{\gamma} = \gamma + \left(Z^{*\top}Z^*\right)^{-1} Z^{*\top}C\epsilon
$$

And the variance of the estimator is:

$$
\mbox{V}(\hat{\gamma}) = \left(Z^{*\top}Z^*\right)^{-1} Z^{*\top}C\Omega^{-1}C^\top Z^*\left(Z^{*\top}Z^*\right)^{-1}
$$ {#eq-glsvar}

But
$C\Omega^{-1}C^\top = C (C^\top C)^{-1} C^\top = C C ^ {-1} C^{\top-1}C^\top=I$[^non_spherical-3],
and therefore, @eq-glsvar simplifies to:

[^non_spherical-3]: $(AB) ^ {-1} = B^{-1} A^{-1}$ if the inverse of the
    two matrix exists, see @GREE:18\index[author]{Greene}, online appendix p 1074.

$$
\mbox{V}(\hat{\gamma}) = \left(Z^{*\top}Z^*\right)^{-1}
$$ 

which is very similar to the formula used for the OLS estimator. Note that
$\sigma_\epsilon^2$ doesn't appear in this formula because the variance
of the transformed errors is 1.

### Weighted least squares

\index[general]{weighted least squares|(}
\index[general]{generalized least squares!weighted least squares|(}

With heteroskedastic, but uncorrelated errors,
$\Omega$ is diagonal and each element is the specific variance of one
observation. From @eq-matheterosc, it is obvious that the transformation
matrix $C$ can be written as:

$$
C= 
\left(
\begin{array}{ccccc}
1 / \sigma_{1} & 0 & 0 & \ldots & 0 \\
0 & 1 / \sigma_{2} & 0 & \ldots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \ldots & 1 / \sigma_N
\end{array}
\right)
$$

and therefore, pre-multiplying any vector by $C$ leads to a transformed
vector where each value is divided by the standard deviation of the
corresponding error:
$z^{*\top} = (z_1 / \sigma_1, z_2 / \sigma_2, \ldots, z_N / \sigma_N)$.
Performing OLS on the transformed data, we get the the **weighted-least square** estimator (**WLS**). The name of this estimator comes from the fact that the estimator can be obtained by minimizing $\sum_n \epsilon_n ^ 2 / \sigma_n ^ 2$, ie by
minimizing not the sum of squares residuals, but a linear combination of
squares residuals, the weight of each observation being $1 / \sigma_n ^ 2$. Therefore, an observation $n$ for which $\sigma_n^2$ is high will
receive a smaller weight in WLS compared to OLS. The weights
are unknown. The simplest solution is to assume that the variance (or the standard deviation) of the errors are
proportional to an observed variable (which may be or not a covariate of
the regression). We then get either $\sigma_n ^ 2 = \sigma ^ 2 w_n$ or $\sigma_n ^ 2 = \sigma ^ 2 w_n ^ 2$ and the weights are respectively $1 / w_n$ or $1 / w_n ^ 2$. The WLS estimator can then be obtained by OLS with all the variables divided either by $\sqrt{w_n}$ or $w_n$. 
A more general solution is to assume a functional form
for the skedastic function: $\sigma_n ^ 2 = h(\delta^\top w_n)$ where
$h$ is a monotonous increasing function
that returns only positive values, $w$ is a set of covariates and
$\delta$ a vector of parameters. If, for example $h$ is the
exponential function (which is a very common choice), the skedastic
function is: $\ln \sigma_n ^ 2 = \delta^\top w_n$ and $\delta$
can be consistently estimated by performing the following regression:

$$
\ln \hat{\epsilon}_n ^ 2 = \delta ^ \top w_n + \nu_n
$$ {#eq-skedeq} 

where $\hat{\epsilon}$ are the OLS residuals which are
consistent estimates of the errors. The WLS estimator is then performed
in three steps:

-   estimate the model by OLS and retrieve the vector of residuals
    $\hat{\epsilon}$,
-   estimate $\hat{\gamma}$ by using OLS on @eq-skedeq and compute
    $\hat{\sigma}_n ^ 2 = e^{\hat{\gamma} w_n}$,
-   divide every variable (the response and the covariates) by
    $\hat{\sigma}_n$ and perform OLS on the transformed variables.

Note that there is no intercept in the third estimation as the
"covariate" associated to the intercept (a vector of 1) becomes a vector
with typical element $1/\hat{\sigma}_n$.

For the electricity consumption example, we know that the unconditional variance of $y$ in city $n$ is $\sigma_{yn} ^ 2 = \sigma_c ^ 2 / I_n$, $I_n$ being the number of consumption units in city $n$ and $\sigma_c ^ 2$ the variance of the individual consumption. Assuming that the same relation applies for the conditional variance, then $\sigma_{\epsilon n} ^ 2 = \sigma ^ 2 / I_n$ and, therefore, the weights ($1 / \sigma_{\epsilon_n} ^ 2$) are proportional to $I_n$. The WLS estimator can then be obtained by computing OLS on series multiplied by $\sqrt{I_n}$:
\idxdata[(]{uk\_elec}{micsr.data}\idxfun{lm}{stats}

```{r}
wls_elec <- lm(I(kwh * sqrt(cust)) ~ sqrt(cust) + I(inc * sqrt(cust)) + 
                 I(1 / mc6 * sqrt(cust)) +  I(gas6 * sqrt(cust)) + 
                 I(cap * sqrt(cust)) - 1, uk_elec)
```

Or more simply by setting the `weights` argument of `lm` to `cust`:
\idxfun{lm}{stats}

```{r}
wls_elec2 <- lm(kwh ~ inc + I(1 / mc6) +  gas6 + cap, uk_elec, 
                weights = cust)
```

Comparing the robust standard errors of the OLS estimator and those of the WLS estimator, we get:
\idxfun{vcov}{stats}\idxfun{vcovHC}{sandwich}\idxfun{stder}{micsr}\idxfun{tibble}{tibble}

```{r}
std_ols <- vcovHC(lm_elec) %>% stder
std_wls <- vcov(wls_elec2) %>% stder
tibble(ols = std_ols, wls = std_wls, ratio = wls / ols)
```

The efficiency gain of using WLS is consequent, as the standard errors reduce by about 25/50% depending on the coefficient.\idxdata[)]{uk\_elec}{micsr.data}

\index[general]{weighted least squares|)}
\index[general]{generalized least squares!weighted least squares|)}

### Error component model {#sec-error_component_gls}

\index[general]{error component model|(}
\index[general]{panel data!error component model|(}
\index[general]{generalized least squares!error component model|(}

The error component model is suitable for panel data or pseudo panel data (data on siblings for example). In the remaining of this section, we'll mention individual means, which is the proper term for panel data, but should be replaced by entity means for pseudo panel data.
Remind that for the error component model, @eq-poweromegaec is a general
formula that can be used to compute any power of $\Omega$ and $C$ is in
this context obtained by taking $r=-0.5$:

$$
C = \Omega ^ {-0.5} = \frac{1}{\sigma_\nu} W + \frac{1}{\sigma_\iota} B = \frac{1}{\sigma_\nu}\left(W + \frac{\sigma_\nu}{\sigma_\iota} B\right)
$$ {#eq-Cerrcomp}

As $W = I - B$, $C$ can also be rewritten as:

$$
C = \Omega ^ {-0.5} = \frac{1}{\sigma_\nu}\left(I - \left[1 -\frac{\sigma_\nu}{\sigma_\iota}\right] B\right) = \frac{1}{\sigma_\nu}(I - \theta B)
$$ {#eq-Cerrcomp2}

with $\theta = 1 - \frac{\sigma_\nu}{\sigma_\iota}$. $\theta$ can be
further writen as:

$$
\theta = 1 - \frac{\sigma_\nu}{\sqrt{T \sigma_\eta^2 + \sigma_\nu ^ 2}} = 1 - \frac{1}{\sqrt{T \sigma_\eta ^ 2 / \sigma_\nu^2+1}}
$$ 
Therefore $0\leq \theta \leq 1$, so that the $\sigma_\nu C$ matrix performs, in
this context, a quasi-difference from the individual mean:

$$
z_n ^ * = z_{nt} - \theta \bar{z}_{n.}
$$

The share of the individual mean that is subtracted depends on:

-   the relative weights of the two variances: $\theta \rightarrow 0$
    when $\sigma_\eta ^ 2 / \sigma_\nu ^ 2 \rightarrow 0$, which means
    that there is no individual effects. As
    $\sigma_\eta ^ 2 / \sigma_\nu ^ 2 \rightarrow + \infty$,
    $\theta \rightarrow 1$ and the transformation is the difference from
    the individual mean,
-   the number of observations for each individual,
    $\theta \rightarrow 1$ when $T\rightarrow + \infty$, therefore the
    transformation is close to a difference from individual mean for
    large $T$.

$\sigma_\nu$ and $\sigma_\eta$ are unknown parameters and have to be
estimated.
Consider the errors of the model $\epsilon_{nt}$, their individual mean
$\bar{\epsilon}_{n.}$ and the deviations from these individual means
$\epsilon_{nt} - \bar{\epsilon}_{n.}$. By hypothesis, we have:
$\mbox{V}\left(\epsilon_{nt}\right)=\sigma_\nu^2+\sigma_\eta^2$. For the
individual means, we get:

$$
\bar{\epsilon}_{n.}=\frac{1}{T}\sum_{t=1}^T \epsilon_{nt} = \eta_n +
\frac{1}{T}\sum_{t=1}^T \nu_{nt}
$$

for which the variance is:

$$
\mbox{V}\left(\bar{\epsilon}_{n.}\right)=\sigma_{\eta}^2 + \frac{1}{T}
\sigma_{\nu}^2 = \sigma_\iota^2 / T
$$

The variance of the deviation from the individual means is easily
obtained by isolating terms in $\epsilon_{nt}$:

$$
\epsilon_{nt} - \bar{\epsilon}_{n.}=\epsilon_{nt}-\frac{1}{T}\sum_{t=1}^T
\epsilon_{nt}=\left(1-\frac{1}{T}\right)\epsilon_{nt}-
\frac{1}{T}\sum_{s \neq t} \epsilon_{ns}
$$

The variance is, noting that the sum now contains $T-1$ terms:

$$
\mbox{V}\left(\epsilon_{nt} - \bar{\epsilon}_{n.}\right) =
\left(1-\frac{1}{T}\right)^2\sigma_{\nu}^2+
\frac{1}{T^2}(T-1)\sigma_{\nu}^2 = \frac{T-1}{T}\sigma_\nu ^ 2
$$
If $\epsilon$ were known, natural estimators of these two variances
$\sigma_{\iota}^2$ et $\sigma_{\nu}^2$ would be:

$$
\hat{\sigma}_\iota^2 = T \frac{\sum_{n=1}^N\bar{\epsilon}_{n.}^2}{N} =
T \frac{\sum_{n=1}^N\sum_{t=1}^T\bar{\epsilon}_{n.}^2}{NT}=T\frac{\epsilon^{\top}B\epsilon}{NT}=\frac{\epsilon^{\top}B\epsilon}{N}
$$ {#eq-varmxt}

$$
  \hat{\sigma}_{\nu}^2 = \frac{T}{T-1}
  \frac{\sum_{n=1}^N\sum_{t=1}^T\left(\epsilon_{nt} - \bar{\epsilon}_{n.}\right)^2}{NT}
  =\frac{\sum_{n=1}^N\sum_{t=1}^T\left(\epsilon_{nt} - \bar{\epsilon}_{n.}\right)^2}{N(T-1)}
    = \frac{\epsilon^{\top}W \epsilon}{N(T-1)}
$$ {#eq-varidios}

Several estimators of the two components of the variance have been
proposed in the literature. They all consist on replacing
$\epsilon_{nt}$ in the previous two equations by consistent estimates
(and for some of them by applying some degrees of freedom correction).
The estimator proposed by @WALL:HUSS:69\index[author]{Wallace}\index[author]{Hussain} is particularly simple because
it uses the residuals of an OLS estimation.[^non_spherical-4]

[^non_spherical-4]: See also @SWAM:AROR:72\index[author]{Swamy}\index[author]{Arora}, @AMEM:71\index[author]{Amemiya} and @NERL:71\index[author]{Nerlove}.

\idxdata[(]{twins}{micsr.data}
The residuals of the OLS regression were already added to the `twins` data set and 
$B \hat{\epsilon}$ was computed. We then compute $W\hat{\epsilon} = \hat{\epsilon} - B \hat{\epsilon}$ and we use @eq-varmxt and @eq-varidios to compute $\hat{\sigma}_\iota ^ 2$, $\hat{\sigma}_\nu ^ 2$, $\theta = 1 - \hat{\sigma}_\nu / \hat{\sigma}_\iota$ and then $\hat{\sigma}_\eta ^ 2 = (\hat{\sigma}_\iota ^ 2 - \hat{\sigma}_\nu^2) / T$.
\idxfun{mutate}{dplyr}\idxfun{summarise}{dplyr}\idxfun{sqrt}{base}\idxfun{sum}{base}


```{r}
#| collapse: true
twins <- twins %>% 
  mutate(We = e - Be)
sigs <- twins %>% 
  summarise(s2iota = sum(Be ^ 2) / N_tw, 
            s2nu = sum(We ^ 2) / (N_tw * (T_tw - 1))) %>% 
  mutate(s2eta = (s2iota - s2nu) / T_tw,
         theta = 1 - sqrt(s2nu / s2iota))
sigs
```

`plm` function performs the GLS estimation, ie an OLS regression on data transformed using quasi-differences ($w_{nt} ^ * = w_{nt} - \theta \bar{w}_{n.}$) if the `model` argument is set to `"random"`:
\idxfun{plm}{plm}\idxfun{summary}{base}

```{r}
gls_twins <- plm(log(earning) ~ poly(age, 2) + educ, twins, 
                 method = "walhus", model  = "random")
summary(gls_twins)
```

Note that we set the `method` argument to `"walhus"` to select the Wallace and Hussain estimator.
The output is quite similar to the one for `lm`' objects, except the two parts that appear at the beginning. The dimensions of the panel are indicated (number of individuals / entities and number of time series / observations in each entity) and whether the data set is balanced or not. A panel data is balanced if all the individuals are observed for the same set of time periods. In a pseudo-panel (which is the case here), the data set is balanced if there is the same number of observations for each entity (which is obviously the case for our sample of twins). This information can be obtained using `pdim`. The first argument is a data frame, the second one, called `index`, is a vector of two characters indicating the name of the individual and of the time index. It can be omitted if the first two columns contains these indexes, which is the case for the `twins` data set:
\idxfun{pdim}{plm}

```{r}
#| collapse: true
pdim(twins, index = c("family", "twin"))
```

The second specific part of the output gives information about the variances of the two components of the error. We can see here that the individual effects (in this example a family effect) account for only 15% of the total variance of the error. Therefore, only a small part of the individual mean is removed while performing GLS (14.3%). This information can be obtained using the `ercomp` function:
\idxfun{ercomp}{plm}

```{r}
#| label: ercomp
#| results: false
ercomp(log(earning) ~ poly(age, 2) + educ, twins, method = "walhus")
```

The model can also be estimated by maximum likelihood, using `pglm::pglm`. This function adapts the behavior of the `stats::glm` function which fits generalized linear model for panel data. In particular, it has a `family` argument that is set to `"gaussian"`:
\idxfun{pglm}{pglm}\idxfun{gaze}{micsr}

```{r}
ml_twins <- pglm::pglm(log(earning) ~ poly(age, 2) + educ, 
                       twins, family = gaussian)
ml_twins %>% gaze
```

$\sigma_\eta$ and $\sigma_\nu$ are now two parameters that are directly estimated. We can see that the estimated values are very close to the ones obtained using GLS and that $\sigma_\eta$ is statistically significant.
Comparing OLS and GLS standard deviations, we get:
\idxfun{vcov}{stats}\idxfun{vcovCL}{sandwich}\idxfun{stder}{micsr}\idxfun{rbind}{base}

```{r}
ols_se <- vcovCL(lm_twins, ~ family, type = "HC0", cadjust = FALSE) %>% stder
gls_se <- vcov(gls_twins) %>% stder
rbind(ols = ord_se, gls = gls_se, ratio = gls_se / ols_se)
```

\idxdata[)]{twins}{micsr.data}
and therefore, there seems to be no gain of efficiency while using OLS instead of GLS. With the Tobin's Q example, we get:
\idxdata[(]{tobinq}{micsr.data} \idxfun{plm}{plm}\idxfun{ercomp}{plm}

```{r}
gls_q <- plm(ikn ~ qn, tobinq, model = "random")
ercomp(gls_q)
```

The share of the individual effect is now 27.5% and the GLS is now OLS on series for which 73.5% of the individual mean has been removed, mostly because the time dimension of the panel is high (35 years). Comparing the robust standard errors of OLS and those of GLS, we get:
\idxfun{lm}{stats}\idxfun{vcovCL}{sandwich}\idxfun{stder}{micsr}\idxfun{vcov}{stats}\idxfun{rbind}{base}

```{r}
ols_q <- lm(ikn ~ qn, tobinq)
sd_ols_q <- vcovCL(ols_q, ~ cusip) %>% stder
sd_gls_q <- vcov(gls_q) %>% stder
rbind(ols = sd_ols_q, gls = sd_gls_q, ratio = sd_gls_q / sd_ols_q)
```

GLS is much more efficient that OLS as the standard error of the slope are 4 about times smaller.
\idxdata[)]{tobinq}{micsr.data}

\index[general]{error component model|)}
\index[general]{panel data!error component model|)}
\index[general]{generalized least squares!error component model|)}

### Seemingly unrelated regression {#sec-sur}
\index[general]{system estimation!seemingly unrelated regression|(}
\index[general]{seemingly unrelated regression|(}
\index[general]{generalized least squares!seemingly unrelated regression|(}

Because of the inter-equations correlations, the efficient estimator is
the GLS estimator:
$\hat{\gamma}=(Z^\top\Omega^{-1}Z)^{-1}Z^\top\Omega^{-1}y$. This
estimator, first proposed by @ZELL:62\index[author]{Zellner}, is known by the acronym **SUR**
for **seemingly unrelated regression**. It can be obtained by applying
OLS on transformed data, each variable being pre-multiplied by
$\Omega^{-0.5}$. This matrix is simply
$\Omega^{-0.5}=\Sigma^{-0.5}\otimes I$. Denoting $\delta_{lm}$ the
elements of $\Sigma^{-0.5}$, the transformed response and covariates
are:

$$
y ^ *=
\left(
  \begin{array}{c}
  \delta_{11} y_1 + \delta_{12} y_2 + \ldots +\delta_{1L} y_L \\
  \delta_{21} y_1 + \delta_{22} y_2 + \ldots +\delta_{2L} y_L \\
    \vdots \\
  \delta_{L1} y_1 + \delta_{L2} y_2 + \ldots +\delta_{LL} y_L
  \end{array}
\right),
Z ^ *=
\left(
  \begin{array}{cccc}
  \delta_{11} Z_1  &\delta_{12} Z_2 & \ldots & \delta_{1L} Z_L \\
  \delta_{21} Z_1  & \delta_{22} Z_2  & \ldots & \delta_{2L} Z_L \\
    \vdots      & \vdots      & \ddots & \vdots \\
  \delta_{L1} Z_1  & \delta_{L2} Z_2  & \ldots & \delta_{LL} Z_L
  \end{array}
\right)
$$ {#eq-transformation_sur}

$\Sigma$ is a matrix that contains unknown parameters, which can be
estimated using residuals of a consistent but inefficient preliminary
estimator, like OLS. The efficient estimator is then obtained the
following way:

-   first, estimate each equation separately by OLS and denote
    $\hat{\Xi} = (\hat{\epsilon}_1, \hat{\epsilon}_2, \ldots,\hat{\epsilon}_L)$
    the $N\times L$ matrix for which every column is the residual vector
    of one of the equations in the system,
-   then, estimate the covariance matrix of the errors:
    $\hat{\Sigma}=\hat{\Xi}^\top\hat{\Xi} / N$,
-   compute the matrix $\hat{\Sigma}^{-0.5}$ and use it to transform the
    response and the covariates of the model,
-   finally, estimate the model by applying OLS on transformed data.

$\Sigma^{-0.5}$ can conveniently be computed using the Cholesky
decomposition, ie the upper-triangular matrix $C$ which is
such that $C^\top C=\Sigma^{-1}$.

<!-- clarifier lower/uper triangular -->

To illustrate the use of the **SUR** estimator, we go back to the
estimation of the system of three equations (one cost
function and two factor shares equations) for the production of apples
started in @sec-system_equation. In this section, we computed a tibble containing the three responses `Y` and the model matrices for the three equations `Z_c`, `Z_l` and `Z_m` (respectively for the cost, the labor and the materials equations). In @sec-bptest_system, we estimated the covariance matrix of the errors of the three equations `Sigma`.
To implement the **SUR** estimator, we compute the Cholesky decomposition of the inverse of the estimated covariance matrix of the errors of the three equations:
\idxfun{transmute}{dplyr}\idxfun{filter}{dplyr}\idxfun{model.frame}{stats}\idxfun{model.matrix}{stats}\idxfun{model.response}{stats}\idxfun{mean}{base}\idxfun{Formula}{Formula}\idxfun{paste}{base}\idxfun{bdiag}{Matrix}\idxfun{as.matrix}{base}\idxfun{model.part}{Formula}\idxfun{pivot\_longer}{tidyr}\idxfun{bind\_cols}{dplyr}\idxfun{nobs}{stats}\idxfun{clm}{micsr}\idxfun{lm}{base}\idxfun{resid}{stats}\idxfun{crossprod}{base}\idxfun{matrix}{base}\idxfun{colnames}{base}

```{r }
#| eval: false
#| include: false
ap <- apples %>% filter(year == 1985) %>%
    transmute(y = otherprod + apples,
              ct = capital + labor + materials,
              sl = labor / ct, sm = materials / ct,
              pk = log(pc / mean(pc)), pl = log(pl / mean(pl)) - pk,
              pm = log(pm / mean(pm)) - pk, ct = log(ct / mean(ct)) - pk,
              y = log(y / mean(y)), y2 = 0.5 * y ^ 2,
              ct = ct, pl2 = 0.5 * pl ^ 2,
              pm2 = 0.5 * pm ^ 2, plm = pl * pm
              )
eq_ct <- ct ~ y + y2 + pl + pm + pl2 + plm + pm2
eq_sl <- sl ~ pl + pm
eq_sm <- sm ~ pl + pm
library(Formula)
eq_sys <- Formula(ct + sl + sm ~ y + y2 + pl + pm + pl2 + plm + pm2)
mf <- model.frame(eq_sys, ap)  ; Z_c <- model.matrix(eq_ct, mf) 
Z_l <- model.matrix(eq_sl, mf) ; Z_m <- model.matrix(eq_sm, mf)
nms_cols <- function(x, label)
    paste(label, c("cst", colnames(x)[-1]), sep = "_")
nms_c <- nms_cols(Z_c, "cost") ; nms_l <- nms_cols(Z_l, "sl")
nms_m <- nms_cols(Z_m, "sm")
Zs <- Matrix::bdiag(Z_c, Z_l, Z_m) %>% as.matrix
colnames(Zs) <- c(nms_c, nms_l, nms_m)
Y <- model.part(eq_sys, mf, rhs = 0, lhs = 1)
ys <- Y %>% pivot_longer(1:3, cols_vary = "slowest", names_to = "equation",
                         values_to = "response")
R <- matrix(0, nrow = 6, ncol = 14)
R[1, c(4,  9)] <- R[2, c(5, 12)] <- R[3, c(6, 10)] <- R[4, c(7, 11)] <-
    R[5, c(7, 13)] <- R[6, c(8, 14)] <- c(1, -1)
stack_data <- ys %>% bind_cols(Zs)
ols_unconst <- lm(response ~ . - 1 - equation, stack_data)
ols_const <- clm(ols_unconst, R)
N_ap <- nobs(ols_const) / 3
EPS <- ols_const %>% resid %>% matrix(ncol = 3)
Sigma <- crossprod(EPS) / N_ap
```

\idxfun{chol}{base}\idxfun{solve}{base}

```{r }
V <- chol(solve(Sigma))
V
```

We then transform the response and the covariates using @eq-transformation_sur:
\idxfun{rbind}{base}\idxfun{as.matrix}{base}\idxfun{t}{base}\idxfun{as.numeric}{base}

```{r}
Zs <- rbind(cbind(V[1, 1] * Z_c, V[1, 2] * Z_l, V[1, 3] * Z_m),
            cbind(V[2, 1] * Z_c, V[2, 2] * Z_l, V[2, 3] * Z_m),
            cbind(V[3, 1] * Z_c, V[3, 2] * Z_l, V[3, 3] * Z_m))
ys <- as.matrix(Y) %*% t(V) %>% as.numeric
```

Then the SUR estimator is computed, using `lm` on the transformed
data and the using `clm` in order to impose the linear restrictions.
\idxfun{lm}{stats}\idxfun{clm}{micsr}\idxfun{coef}{stats}

```{r }
sur <- lm(ys ~ Zs - 1) %>% clm(R = R)
sur %>% coef
```

More simply, the `systemfit::systemfit` function can be used, ^[The **systemfit** package was presented in @sec-constrained_ls.] with the `method` argument set to `"SUR"`:^[Note the use of the `methodResidCov` argument: setting it to `"noDfCor"`, the cross-product of the vectors of residuals is divided by the number of observations to get the estimation of the covariance matrix. Other values of this argument enables to perform different kinds of degrees of freedom correction.]
\idxfun{systemfit}{systemfit}

```{r }
library(systemfit)
sur <- systemfit(list(cost = eq_ct, labor = eq_sl, materials = eq_sm),
                 data = ap, restrict.matrix = R, method = "SUR", 
                 methodResidCov = "noDfCor")
```

The coefficients of the fitted model can be used to compute the Allen elasticities of
substitution and the price elasticities. The former are defined as:

$$
\sigma_{ij} = \frac{\beta_{ij}}{s_i s_j} - 1 \; \; \forall i \neq j
\mbox{ and } \sigma_{ii} = \frac{\beta_{ij} - s_i(1 - s_i)}{s_i ^ 2}
$$

Denote $B$ the matrix containing the coefficients $\beta_{ij}$. Remind that, imposing the homogeneity of degree one of the cost function, we imposed that $\beta_{iI} = - \sum_{i=1}^{I-1}\beta_{ij}$. Therefore $\beta_{iI}$ was not estimated and we must added to the $B$ matrix using this formula:
\idxfun{matrix}{base}\idxfun{coef}{stats}\idxfun{apply}{base}\idxfun{cbind}{base}\idxfun{rbind}{base}\idxfun{mean}{base}\idxfun{sum}{base}\idxfun{diag}{base}\idxfun{outer}{base}\idxfun{list}{base}\idxfun{dimnames}{base}

```{r }
B <- matrix(coef(sur)[- (1:8)], ncol = 2)[-1, ]
add <- - apply(B, 1, sum)
B <- cbind(rbind(B, add), c(add, - sum(add)))
shares <- ap %>% summarise(sl = mean(sl),
                           sm = mean(sm), sk = 1 - sl - sm) %>%
    as.numeric
elast <- B /outer(shares, shares) + 1
diag(elast) <- diag(elast) - 1 / shares
dimnames(elast) <- list(c("l", "m", "k"), c("l", "m", "k"))
elast
```

The three factors are substitutes all the Allen elasticities of
substitution being positive. 
The price elasticities are given by: $\epsilon_{ij} = s_j \sigma_{ij}$.
\idxfun{rbind}{base}

```{r }
elast * rbind(shares, shares, shares)
```

Note that this matrix is not symmetric: for example $0.1675$ is the
elasticity of the demand of materials with the price of labor as $0.1039$
is the elasticity of the demand of labor with the price of materials.
The price elasticities indicate that the demand for the three inputs are
inelastic, and it is particularly the case for labor and materials.

\index[general]{system estimation!seemingly unrelated regression|)}
\index[general]{seemingly unrelated regression|)}
\index[general]{generalized least squares!seemingly unrelated regression|)}
\index[general]{generalized least squares|)}
\idxdata[)]{data}{micsr.data}
